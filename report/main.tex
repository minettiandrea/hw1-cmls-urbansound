\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{biblatex}
\usepackage{listings}
\usepackage{comment}
\addbibresource{references.bib}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\urlstyle{same}

\title{CMLS Homework 1 - Assignment 4 - Audio Event Classification}
\author{10751302 - 10727803 - 10455526 - 10723074}
\date{2nd Sem. A.A. 2019/2020}

\usepackage{graphicx}

\DeclareCiteCommand{\cite}{}{}{}{}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

\begin{document}

\maketitle

\section*{The GitHub Repository}
The GitHub Repo related to this project is available at this link:\break \hyperlink{https://github.com/minettiandrea/hw1-cmls-urbansound/}
{https://github.com/minettiandrea/hw1-cmls-urbansound/}

\section*{Introduction}
The assignment that has been provided to our group for the CMLS subject is a Classifier that works on a specific dataset that contains recordings from some urban sound, such as air conditioner, car horn, jack hammer and others. The idea is to implement a classifier that automatically recognises these specific sounds, placed in casual order.

The workflow is basically a Supervised Machine Learning classification: we own a metadata file in which are collected both Features and Targets. The metadata file will be our features data table or data model. The aim is to feed the data to a training algorithm that builds a data model. In our case we use al classifier because we are interested to classify data into categories, while a regressor would insteas be used for numerical variables (not our case).
As every basic Machine Learning algotithm we are called to use the two API's Fit (model training) and Predict (given some inputs, compute a likely acceptable output).

The dataset used is the URBANSOUND8k dataset, a dataset from an open source project, free to download, made available for academic/research purposes. This dataset contains:
\begin{itemize}
  \item 8732 labeled sound excerpts with a lenght $<= 4s$
  \item a metadata file, called UrbanSound8k.csv that contains meta-data information about every audio file in the dataset.
\end{itemize}
The sound excerpts are taken from www.freesound.org and are already pre-sorted into ten folds for cross-validation.The link of the dataset is available in the references list at the page bottom.

\section*{Workflow}
Basically what we need to do is to have a look first at the aim of the project and then to the dataset provided, in order to have a better idea of what we have to work on.
The audio data has been already excerpted and even allocated to 10 different folders. Looking into the documentation we discover that some of the excerpts are from the same original file but different sliced.
After a brief overview we discover that this dataset is barely unbalanced, in particular car\_horn, gun\_shot which have a bit less than half amount of entries compared to other 8 classes.

We then had to choose a proper feature extraction method. In order to do so we researched the net some good options suitable for our work.
\begin{itemize}
  \item An interesting GitHub repository containing script for Sound Classification using Librosa, ffmpeg, CNN, Keras, XGBoost and Random Forest. \hyperlink{https://github.com/ravising-h/Urbansound8k}{Open this resource}
  \item We had a read to a not so easy but greatly working Sound Classificator called GMM. \hyperlink{https://towardsdatascience.com/gaussian-mixture-modelling-gmm-833c88587c7f}{Open this resource}
  \item Another interesting GitHub repository that contains 3D-CNN+RNN implementation for bird audio detection and recognition. \hyperlink{https://github.com/himaivan/BAD2}{Open this resource}
\end{itemize}

After these readings we decided to use \emph{MFCC Extraction}, a powerful feature contained into the \emph{LIBROSA} Python package, a package specifically developed for sound, music and audio analysis.
\newline
\newline
Into the $main()$ we created a run\_id that automatically give a file name to the confusion matrix of that specific run. All our run confusion matrix take place into the run folder in the GitHub related page.
Right after we open the \emph{metadata file} and we organize all the data contained, so we are going to have a full reading of both training and test data. After when the feature extraction of the parameters are set we calculate all the features of all the files. We decided to put this set of instruction at the beginning of the code because are the most expensive in terms of CPU computation.
\newline
\newline
Into the $run\_folder(folder)$ function we get the training set and the data set for every folder with a recursive behaviour. Then we defined and trained the model and computed the accuracy as every machine learning algorithm tells us.\break

\textbf{Feature estraction}
Into the $sound.py$ file we find the $feature\_estraction()$ function. Here we compute in order:
\begin{itemize}
\item MFCC Estractrion, using the parameters passed by FeatureEstractionParameters
\item Chroma Features
\item Zero Crossing Rate
\item Spectral Centroid and Bandwidth
\item Root Main Square 
\end{itemize}
And we concatened everything into an array that will contain all the features listed above.
\newline
\newline
\textbf{Calculate the mean}
The work is semplified because we are using only samples that are already perfectly cropped. We are not performing event detection, so we are not interested on where the sample starts or stops. \emph{That's way we computed an averaging (mean) between all the coefficients of the audio frames trough Windowing method}. The aim is to get 13 coefficients of MFCC, but without averaging we would have get 13 coefficients for every time frame, that is computationally expensive and particularly useless.


\section*{Improvements}
After the first runs we got some very poor result, in the order of 45/50\% of accurancy. Unfortunately quite low compared to the documentation we read.
\begin{itemize}
\item We discovered that varying the $hop\_lenght$ of MFCC was having a little impact on the accurancy, but after increasing the $n\_mfcc$ parameter to the value \emph{25} we gained a 4,5\% of accurancy, ending up with a 54,6\% of tota accurancy. Bringing then up the $n\_mfcc$ value to \emph{30} we end up with a value that is really close to the previsious one. In the repo is possibile to check the CSV file with all the data.
\item We then managed to obtain a 60\% of accurancy by adding to the audio features the \emph{Chroma Features}, as ZCR, SC, SB and RMS for each audio, obtaining a total 13 coefficients for every audio (as discussed above).
\item We tryed out different SVM parameters but we ended up that the default ones were giving the best results.
\item Performing web researches we discovered finally that doing \emph{predictions} on the \emph{training set} would provide the opportunity to get up to a 80/90\% of accuracy, due to an overfitting issue.
\item One of our improvements was to rent a powerful Amazon server and give to the code an amendment in order to run the code into it, experiment a very great time save.
\item We then pushed a commit that makes the computation \emph{Multithreading} regarding the feature estraction (the most CPU intensive), obtaining an overall computation time of 10/15 minutes for a modern machine. The code we are referring to is the one below:
%multithread computation
\begin{lstlisting} [language = Python]
    def calculate_all_features(self,params:FeatureExtractionParameters):
        def process(s:Sound):
            s.feature_extraction(params)
        
        # n_jobs=1 means: use all available cores
        Parallel(n_jobs=-1, backend='threading', verbose=10)(delayed(process)(node) for node in self.__metadata)
\end{lstlisting}
\end{itemize}

\section*{Data regrouping}
\textbf{ !!!!!!!!!!!!!!! insert confusion matrix !!!!!!!!!!!!!!!!}

\section*{Conclusion}
\textbf{ !!!!!!!!!!!!!!! write conclusions !!!!!!!!!!!!!!!!}
\newline
\newline
\newline
The following part collects all the references to the documentation consulted by us for the preparation of this homework.

\cite{urbansound}
\cite{crossvalidation}
\cite{librosa}
\cite{librosamfcc}
\cite{usc}
\cite{suplearning}
\cite{gmm}
\cite{dcase}
\cite{bad2}
\cite{bird}

\printbibliography

\end{document}
